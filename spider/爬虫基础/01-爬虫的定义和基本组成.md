爬虫



### 简介

​	网络爬虫（web crawler）， 以前经常称之为网络蜘蛛（spider）， 是按照一定的规则自动浏览万维网并获取信息的机器人程序（或脚本）, 曾经被广泛的应用于互联网搜索引擎。网络爬虫系统正是通过网页中的超链接信息不断获取网络上的其他页面。



### 爬虫的基本组成

​	爬虫通常分为数据采集（网页下载）、数据处理（网页解析）和数据存储（有用的信息持久化）三个部分。

工作流程：

1. 设定抓取目标（种子页面/起始页面）并获取网页。
2. 当服务器无法访问时，按照指定的重试次数尝试重新下载页面。
3. 在需要的时候设置用户代理或隐藏真实IP，否则可能无法访问页面。
4. 对获取的页面进行必要的解码操作然后抓取出需要的信息。
5. 在获取的页面中通过某种方式（如正则表达式）抽取出页面中的链接信息。
6. 对链接进行进一步的处理（获取页面并重复上面的动作）。
7. 将有用的信息进行持久化以备后续的处理。



#### Urllib使用



浏览器中的 NETWORK-XHR 查看网页的接口信息



查看动态加载的页面（通过 Ajax 动态加载的页面）

- selenium(自动化测试框架)





#### 数据管理

- 存储：mysql、redis、mongodb、sqlalchemy
- 序列化： json
- 调度器：进程、线程、协程



#### 请求头

- accept  接受数据的参数   */\*  代表任何都能接收
- Accept-Encoding : gzip
- Accept-Language : 语言
- Connection : 连接状态   keep-alive
- Cookie
- Host  主机
- Referer : 引用   代表网站流量的来源

- ==User-Agent== : 代表当前浏览器的版本， 告诉服务器请求是从浏览器过来的， 是正常请求， 防止反爬虫的禁止访问





#### urllib.request

```python

from urllib import request


url = 'https://www.baidu.com'

# 添加请求头的用户代理
header = {
    'User-Agent': r'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'
}

req = request.Request(url=url, headers=header)

res = request.urlopen(req)
print(res.read())
```



#### ssl证书

- SSL证书就是遵守 SSL 安全套接层协议的服务器数字证书
- 而 SSL 安全协议最初是由美国网景 Netscape Communication 公司设计开发的，全称为：安全套接层协议 (Secure Sockets Layer) ， 它指定了在应用程序协议 ( 如 HTTP 、 Telnet 、 FTP) 和 TCP/IP 之间提供数据安全性分层的机制，它是在传输通信协议 (TCP/IP) 上实现的一种安全协议，采用公开密钥技术，它为 TCP/IP 连接提供数据加密、服务器认证、消息完整性以及可选的客户机认证。由于此协议很好地解决了互联网明文传输的不安全问题，很快得到了业界的支持，并已经成为国际标准。
- SSL 证书由浏览器中“受信任的根证书颁发机构”在验证服务器身份后颁发，具有网站身份验证和加密传输双重功能。
- 如果能使用 https:// 来访问某个网站，就表示此网站是部署了SSL证书。一般来讲，如果此网站部署了SSL证书，则在需要加密的页面会自动从 http:// 变为 https:// ，如果没有变，你认为此页面应该加密，您也可以尝试直接手动在浏览器地址栏的http后面加上一个英文字母“ s ”后回车，如果能正常访问并出现安全锁，则表明此网站实际上是部署了SSL证书，只是此页面没有做 https:// 链接；如果不能访问，则表明此网站没有部署 SSL证书。



例：忽略 ssl 认证

```python

import ssl
from urllib import request


url = 'https://www.123306.cn/'

header = {
    'User-Agent': r'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'
}

context = ssl._create_unverified_context()  # 忽略未经审核的 ssl 证书认证
req = request.Request(url=url, headers=header)

res = request.urlopen(req, context=context)
print(res.read())
```

SSL相关问题。在使用`urlopen`打开一个HTTPS链接时会验证一次SSL证书，如果不做出处理会产生错误提示“SSL: CERTIFICATE_VERIFY_FAILED”，可以通过以下两种方式加以解决：

- 使用未经验证的上下文

  ```python
  import ssl
  
  request = urllib.request.Request(url='...', headers={...}) 
  context = ssl._create_unverified_context()
  web_page = urllib.request.urlopen(request, context=context)
  ```

- 设置全局的取消证书验证

  ```python
  import ssl
  
  ssl._create_default_https_context = ssl._create_unverified_context
  ```


#### urllib中的编码和解码

`parse.urlencode({'wd': msg})`







