分布式爬虫的要点



核心配置： 将调度器的类和去重列表的类替换为 Scrapy-redis 提供的类，在settings.py添加如下设置

```python
# 配置连接Redis
REDIS_HOST = 
REDIS_PORT = 
REDIS_PASSWORD = 

# 连接mongodb
MONGO_URI = 'mongodb://user:password@127.0.0.1:27017'

# 调度器
SCHEDULER = 'scrapy_redis.scheduler.Scheduler'
# 去重队列
DUPEFILTER = 'scrapy_redis.dupefilter.RFPDupefilter'

# 调度队列  三选一
SCHEDULER_QUEUE_CLASEE = 'scrapy_redis.queue.PriorityQueue'   # 优先级队列
SCHEDULER_QUEUE_CLASEE = "scrapy_redis.queue.FifoQueue"  # 先进先出队列
SCHEDULER_QUEUE_CLASEE = 'scrapy_redis.queue.LifoQueue'  # 后进先出队列 

# 持久化，不清空爬去队列的指纹
SCHEDULER_PERSIST = True  # 默认是 False 即爬虫中断后会自动清空

# 爬虫中断后是继续上次的爬取，还是重新爬取， True 表示会重新爬取
# 分布式爬虫一般不适用此配置，直接使用默认的False
SCHEDULER_FLUSH_ON_START = True
```



#### 优化：

- 使用 Bloom Filter 对接

Bloom Filter ：空间利用率高，节省存储空间

`pip install scrapy-redis-bloomfilter`

```python
# 更改去重队列
DUPEFILTER = 'scrapy_redis_bloomfilter.dupefilter.REPDupefilter'
# 散列函数的个数，更具去重的量剋以自行调整
BLOOMFILTER_HASH_NUMBER = 6
# Bloom Filter的 bit 参数，默认是30， 占用128M内存， 去重量级 1亿
BLOOMFILTER_BIT = 30
```



#### 部署

- 可以使用 Scrapyd 部署操作爬虫
- 也可以使用Scrapyd对接Docker批量的部署爬虫

