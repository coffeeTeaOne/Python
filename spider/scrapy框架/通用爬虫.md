通用爬虫

主要时通过 CrawlSpider, 定义一些爬去的规则来实现页面的提取

#### CrawlSpider

- 它时继承自 Spider 类，除了spider的所有方法和属性之外，它还提供了几个特殊的属性

**rules**

- 爬去规则属性，包含一个或者多个Rule的对象的列表，每个 Rule 对爬取网站的动作都做了定义

**parse_start_url**

- 它时一个可以重写的方法，当start_urls里对应的Request得到Response时，该方法被调用，它会分析Response并返回Item对象或者Request对象



#### **定义rule时的参数**

```python
class scrapy.contrib.spiders.Rule(link_extractor, callback=None, follow=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)
```

link_extractor:

- 定义爬虫需要爬去的链接，时LinkExtractor对象

callback:

- 回调函数，应该避免使用parse()函数作为回掉，避免覆盖原函数

cb_kwargs:

- 字典，它包含传递给回掉函数的参数

follow:

- 布尔值，即True或False，它指定根据该规则从Response提取的链接是否需要跟进，如果callback参数为None，follow默认设置为True，否则默认是False

process_links:

- 指定处理函数，从link_extractor中获取到链接列表时，该函数会调用

process_request:

- 指定处理函数，根据Rule提取到每个Request时，该函数将会调用，对Request进行处理，该函数必须返回Resquest或者None

#### **Item Loader 提取item**

- 提供一种便捷的方法机制来帮助我们方便的提取Item

```python
class scrapy.loader.ItemLoader([item, selector, response, ] **kwargs)
```

**item**:

- 时 Item 对象，可以调用 add_xpath()、add_css、add_value()等方法来填充Item对象

**selector**:

- Selector对象，用来提取填充数据的选择器

**response**：

- Response对象，用于构建选择器的 Response

例如：

```python

from scrapy.loader import ItemLoader
from project.items import Product

def parse(self, response):
    loader = ItemLoader(item=Product(), response=response)
    loader.add_xpath('name', '//div[@class="name"]')
    loader.add_css("stock", 'p#stock')
    loader.add_value('last_updated', 'today')
    return loader.load_item()
```

ItemLoader每个字段都可以包含一个Input Process和Output Process 输入输出处理器，Input Process 收到数据时会立刻对数据进行处理，并保存在itemloader中，当所有数据收集完后，调用load_item方法填充item对象，调用时会先调用Output Processor来处理之前收集的数据，再存入Item中

Processor

- Identity：不处理，返回元数据
- TakeFirst： 返回列表的第一个非空值
- Join：拼接，类似字符串的拼接，默认时使用空格，也可以自定义拼接方法
- Compose：处理字符串，用给定的多个函数处理，依次会经过每个函数处理，如：`Compose(str.upper, lambda s: s.strip())`
- MapCompose：迭代处理结果的列表，其他和 Compose相同
- SelectJmes：可以查询 Json，传入值为 Key，返回 value `pip install jmespath`



出自：崔庆才 Python3 网络爬虫开发实战 第13 章