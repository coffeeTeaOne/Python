# 机器学习常用算法小结

#### 有监督

- 有答案的
- 商用最多的，主要是分类

#### 无监督

- 没有答案

#### 半监督

- 部分有答案
- 使用有答案的数据进行训练模型，然后使用陌生数据进行验证

##### 过拟合和欠拟合

- 过拟合：使用样本的特征过多，导致很多无用的特征被加入到计算中，导致模型泛化受到过多无用特征的影响，精度变得很低
- 欠拟合：在选取特征时，选取的过于简单，主要的特征没有获取，导致模型在测试集上的表现很差

#### kNN

k近邻算法

##### 距离

- 抽象的问题，采用欧式距离最近的点为预测目标的类别

参数：

- `n_neighbors`表示预测是参考的样本的数量，数量越少会导致欠拟合，过多会导致过拟合，选择适当的数量可以是模型达到最好的效果





### 岭回归和Lasso回归

- 实践中，一般首选岭回归，但是如果特征很多，但是只有其中的几个是重要的，那么选择Lasso回归可能效果会更好
- sklearn还提供了 **ElasticNet** 类，结合了 Lasso 和 Ridge 的惩罚项，实践中，这种结合的效果是最好的，不过需要调节俩个参数，一个用于 L1,一个用于 L2

#### 岭回归 Ridge

- 最小二乘法

岭回归是加了二阶正则项的最小二乘， 主要是用于过拟合严重和存在多重共线性，岭回归是有 bias 的，这里的 bias 是为了 vanance 更小

- bias：
- vanance：

引入偏差，让数据便于计算

会将无效的系数无限向0压缩，减少无效系数的影响

##### 参数 alpha

- alpha 为惩罚项
- 越大，系数压索越严重， 原始系数对结果的影响变小
- alpha=0  变为线性回归
- 越小  原始系数对结果的影响增大



#### Lasso 回归

- 使用的是拉格朗日乘法

和岭回归的区别：

- 和岭回归类似，算法不同
- 直接对系数进行修正，而岭回归则是对系数的压缩，Lasso回归会将无用的系数修正为0， 而岭回归只会无限压缩，让系数趋近于0

- lasso 可以将系数压缩到0，压缩系数的强度大于岭回归
- 引入惩罚项![53207144981](./assets/1532071449816.png)，对参数 w 增加了限定条件
- 可以认为lasso是岭回归的加强，岭回归能解决的问题也可以使用lasso回归来解决，可以通过对比模型的精度来确定哪种模型更加可靠

##### 参数 alpha：

- 越大，惩罚力度越小，原始系数对结果的影响越小
- 越小，惩罚力度越大，原始系数对结果的影响越大



### 常见的线性分类算法

- 常见的两种线性分类算法是 Logistic 回归 和 线性支持向量机 SVM



#### 逻辑斯蒂回归  Logistic

根据数据对分类的边界线建立回归公式，目的主要是用于分类，属于**[梯度下降算法](https://blog.csdn.net/u013709270/article/details/78667531)**

**优缺点**：

- 优点：实现简单，易于理解和实现，计算代价不高，速度快，存储资源低
- 缺点：容易欠拟合，分类的精度可能不高

`from sklearn.linear_model import LogaisticRegression`

以回归的算法实现数据的分类

##### 参数 C

- C 越大， 正则化越小，那么模型更强调使系数向量（w）接近0
- C 值越大，LogisticRegression 和 LinearSVC 将尽可能的将训练数据集拟合到最好，

- 较大的 C 值更强调每个数据点都分类正确的重要性

强正则化的模型会选择一条相对水平的线，C值稍大，模型更加关注两个分类错误的样本，C值非常大时，决策边界的斜率会非常的大，模型可能会变得过拟合

#### SVM

主要针对**小样本**数据进行学习、预测（有时也叫回归）的一种方法，能解决神经网络不能解决的问题，而且有很好的泛化能力

#### svc

支持向量机将数据映射到高维空间有两种常用的方法

- 一种是多项式核，在一定阶数内计算原始特征所有的可能多项式
- 另一种是径向基函数核，也叫高斯核，它可以解释为考虑所有的结束的所有可能的多项式，但阶数越高，特征的重要性越小

##### 参数

- C : 正则化参数，与线性模型类似，它限制每个点的重要性， 也就是每个点的 dual_coef_
- gamma：用于控制高斯核的宽度



### 决策树（decision tree）

- 决策树是一种树结构，其每个节点代表一个属性

决策树：信息论

逻辑斯蒂回归和贝叶斯：概率论



构造决策树：

- 分裂属性，二叉树（二分裂）

算法:ID3

- ID3介绍： 就是每次需要分裂时，计算每个属性的增益率，然后选择增益率最大的进行分裂，
- 在决策树中，设D为用类别对训练元祖进行划
- 原则：将无序的数据变得有序
- 熵：信息的混乱度
- 信息熵--离散随机事件出现的概率，系统有序化程度的度量
- 信息增益：以不同属性对数据进行训练划分之后，得到的不同信息熵进行相减，信息增益就是两者的差值。



信息增益越大，说明该属性对结果的影响越大, 贪心算法会选择增益最大的属性进行计算



### 贝叶斯

概率论

在B发生的情况下A发生的概率，条件概率

#### 朴素贝叶斯

- 独立性假设，假设各个属性不想关，进行分类，朴素贝叶斯和线性的分类器是相似的，但是朴素贝叶斯的泛化性能可能会稍差一些



##### 3种贝叶斯模型

1. 高斯分布朴素贝叶斯

```python
from sklearn.naive_bayes import GaussianNB

g_NB = GaussianNB()
g_NB.fit(X_train, y_train)
```



2. 多项式分布朴素贝叶斯

- 用于文本数据

3. 伯努利分布朴素贝叶斯

- 用于文本数据，但是表现不如多项式，适合小文本，精确度高



### K-Means  均值聚类

- 是一种无监督学习的模型
- 自动将相似的对象归一到同一个簇中
- 主要通过不断地取离种子点最近均值的算法
- 算法思想：按照样本之间的距离大小， 将样本分为K个簇，让簇内的店尽量紧密的连在一起，并且让簇之间的距离尽量的大

**原理：**

- 计算各个点之间的欧式距离的和
- K 为分类的簇的个数，K需要事先给定，但是K值的大小是非常难以估计的
- 内置的 ISODATA算法通过类的自动合并和分裂，使得较为合理的类型数目K ，优化算法
- K-Means算法需要用初始随机种子来让每次聚类都在同一位置开始



**流程：**

- 从数据中选择 K 个对象作为初始聚类中心
- 计算每个聚类对象到聚类中心的距离来划分
- 在次计算每个聚类中心
- 计算标准测度函数，知道达到最大迭代次数，则停止，否则，继续操作
- 确定最优的聚类中心